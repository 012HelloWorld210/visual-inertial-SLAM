# 第一次理解

## 1.测试视频设备

​	简单测试，无涉及。

## 2.特征检测与匹配

### ORB特征检测

#### 概述

​	ORB（Oriented FAST and Rotated BRIEF，带方向的FAST和旋转的BRIEF），其是建立在特征检测方法fast（Features from Accelerated Segment Test，加速分割测试特征）的特征检测方法。

#### fast

​	FAST算法通过检查图像中的某像素点周围是否包含足够数量的与该点有明显的亮度差的像素点来确认是否为角点。具体而言，先确定一个像素点，并以此点为中心画一个圆（通常选择半径为3的圆，即检查其周围16个像素点），如果存在N个连续的像素点（一般N取值为9）均比中心点亮，或均比中心点暗，则该点就被认定为是一个角点。详细如下图所示：

<img alt="fast角点" src="D:\postgraduate\长期事件\Github\visual-inertial-SLAM\image\Fast角点.jpg"/>

其步骤包括：1）在图像中任选一点p， 假定其像素（亮度）值为 Ip。

 2）以r为半径画圆，覆盖p点周围的M个像素，如图3.1所示: r=3， M=16。

 3）设定阈值t，如果周围16个像素中有连续的N个像素的像素值小于 Ip−t 或者有连续的N个像素都大于 Ip+t ，则该点就被判断为角点。

#### orb

​	在fast基础上，使用质心公式计算出图片的质心坐标

![质心公式](D:\postgraduate\长期事件\Github\visual-inertial-SLAM\image\质心公式.png)

​	式中，（X,Y）为像素点坐标，I（X,Y）为该点灰度值

​	而orb中的方向即质心坐标指向角点坐标的方向；有此方向后，即使在后续图片进行旋转，也可以将坐标进行对应的旋转从而保持区域内的像素特征不变化（主要为后续的特征子描述做准备）。

​	描述子是在求取角点位置信息后，通过统计角点所在区域（大小通常可自定）像素点的特征，如直方图等来对于角点所在区域进行描述，称为描述子。而BRIEF（Binary Robust Independent Elementary Features，二值独立基本特征）描述子具体实现为：

​	1）关键点检测：使用如FAST等关键点检测算法在图像找到感兴趣点。

​	2）图像补丁提取：在每个关键点周围提取一个小的图像补丁，即选择区域。

​	3）二元比较对选择：在提取的图像补丁内，通过预定义或通过方法优化，随机选择一组像素对（通常是128或256对）。

​	4）生成二进制描述符：对于每对像素，比较其灰度值，如第一个像素的灰度值小于第二个像素，则该对对应的位设为1，否则设为0。结果每个补丁可生成一个二进制字符串，称为该关键点的描述符。

### 汉明距离的BFMatcher

​	BFMatcher（Brute-Force Matcher）是一种暴力匹配的方法，可以使用不同的距离度量标准，其中汉明距离（Hamming Distance）常用于二进制描述符（如ORB、BRIEF等）的匹配。

​	汉明距离计算两个二进制字符串之间不同位的数量，用于衡量它们的相似度。BFMatcher使用汉明距离来找到在两张图像中最相似的特征点，从而实现图像匹配和对齐。

具体流程如下所示：

1. **遍历图片1的描述符**：

   对于每一个来自图片1的描述符（如 `d1`），`BFMatcher` 会执行下面的操作。

2. **逐一配对**：   

    `BFMatcher` 会将这个描述符 `d1` 与图片2中的每一个描述符（如 `d2`）进行比较，计算它们之间的距离（通常是欧氏距离）。

3. **选择最近邻**：  

   从所有计算出的距离中，选择最小的那个，即找到与 `d1` 距离最近的描述符 `d2`。

4. **记录匹配结果**：

      - 将该对描述符（`d1` 和 `d2`）作为一个匹配结果存储起来，包含实际的描述符索引和距离值。

5. **重复过程**： 

      对图片1中的每一个描述符重复上述过程，直到所有描述符都完成匹配。

​    这种方法确保了每个描述符都能找到在另一个图像中的最佳匹配，但也意味着计算量较大，特别是对于包含大量特征点的图像，匹配计算会非常耗时。

​    为了加速匹配过程或者提高准确性，实际应用中常常会使用一些优化和过滤方法，如：

​    \- **K近邻匹配（KNN matching）**：除了最近邻，还会找出前K个最近的描述符，通过进一步筛选来提高匹配准确度。

​    \- **交叉验证匹配（Cross-checking）**：匹配必须是对称的，即如果描述符1匹配到描述符2，那么描述符2也应该匹配回描述符1。

​    \- **比值测试（Ratio test）**：应用在KNN匹配中，通过比较最近的两个距离的比值来过滤掉一些错误匹配。

## 3.RANSAC优化

​	通过RANSAC求取单映射矩阵，然后将之前筛选出来匹配的点代入，即可求出单映射矩阵的误差，此时可设置阈值，将误差较大点筛除，不过前提是单映射矩阵整体误差最小。

### 获取位置

```
[ keypoints1[m.queryIdx].pt for m in matches ]
```

​	首先从maches中获得匹配的特征点信息，此处的m.queryIdx即为第一张图片特征点中的匹配点索引，通过.pt即可获得其对应的二维坐标。

### RANSAC估计几何变换矩阵

```
M, mask = cv2.findHomography(pts1, pts2, cv2.RANSAC, 5.0)
```

​	此处指定模型为单映射矩阵，通过RANSAC方法进行迭代随机采样和评估模型质量，误差阈值为设置为5。返回值M为对应的单映射矩阵；mask返回 指示哪些点是内点（inliers）或外点（outliers），具体来说：

\- `mask` 中的值为 `1` 的位置表示该点对是内点，即在计算单映射矩阵时，被认为是符合预期变换模型的可靠点。

\- `mask` 中的值为 `0` 的位置表示该点对是外点，即被认为是误差较大的点，可能是由于噪声或错误匹配导致的。

#### 单映射矩阵

​	一种用于描述两个平面之间对应关系的3x3变换矩阵。在计算机视觉和图像处理领域，单映射矩阵通常用于将一个图像中的点映射到另一个图像中的相应点。

![单映射矩阵](D:\postgraduate\长期事件\Github\visual-inertial-SLAM\image\单映射矩阵.jpg)

​	X是空间中的一点，左右两边是射影平面（摄像头）。单应是关于三维齐次矢量的一种线性变换，可以用一个3×3的非奇异矩阵H表示，这个矩阵H称为**单应矩阵**。使用这个矩阵，就可以将射影平面上的一个点投影到另一个平面上（图中的 *m* 投影到 *m‘*）。

![关联矩阵](D:\postgraduate\长期事件\Github\visual-inertial-SLAM\image\关联矩阵.jpg)

## 4.匹配中估计相机姿态

<img src=".\image\求解姿态.png" alt="姿态求解" style="zoom:40%;" />

### 相机内参矩阵

​	相机成像的本质：就是三维空间坐标到二维图像坐标的变换，这是一个投影过程。相机矩阵就是建立这种三维到二维的投影关系。

#### 成像原理

焦点：平行于主光轴的光线穿过透镜时， 会聚到一点上， 这个 点叫做焦点， 焦点到透镜中心的距离叫做焦距 f。

### 基础矩阵

#### 概述

​	它描述了两个视图之间的几何关系，特别是当从一个视图中的点到另一个视图中的点的映射。

#### 本质

基础矩阵的本质是描述两个图像之间的极几何关系。极几何关系包括以下概念：

1. **极点（Epipoles）**：
   - 对于任意一点 x1在第一张图像中，其对应点 x2在第二张图像中所形成的直线集合叫做极线。
   - 极点 e2是所有极线在第二张图像中的交点。同样，极点 e1是所有极线在第一张图像中的交点。
2. **极线（Epipolar Lines）**：
   - 极线是通过图像中某个点及其对应极点的直线。在第二张图像中，对于第一张图像中的点 x1，对应的极线是 l2=Fx1。
   - 反之，在第一张图像中，对于第二张图像中的点 x2，对应的极线是 l1=FTx2。

#### 计算

基础矩阵可以通过多对匹配点来计算，通常使用八点算法（Eight-point Algorithm）或其改进版本。以下是一个简化的计算过程：

1. **数据收集**：获取至少八对匹配的点对 (x1,x2)。

2. **构建线性方程组**：根据每对匹配点建立方程
   $$
   x_2^TFx_1 = 0
   $$

3. **求解基础矩阵**：通过线性代数方法（如SVD分解）求解上述方程组，得到基础矩阵 F。

### 本质矩阵

#### 概述

​	计算机视觉和多视几何中的一个重要概念。它用于描述两个视图之间的几何关系，特别是在已知相机内参的情况下。

#### 本质

​	本质矩阵的本质是描述两个图像之间的极几何关系，但它与基础矩阵不同，因为它假设相机的内参已知。也就是说，本质矩阵是基础矩阵在相机内参已知情况下的特例。

1. **极点（Epipoles）和极线（Epipolar Lines）**：

   - 和基础矩阵一样，本质矩阵也描述了极点和极线的关系。
   - 极点和极线的概念与基础矩阵中的定义相同。

2. **与基础矩阵的关系**：

   - 如果已知相机的内参矩阵 K1和 K2，基础矩阵 F 和本质矩阵 E 之间的关系为：

   $$
   E = K_2^TFK_1
   $$

3. 

### 姿态解算

#### 概述

​	本质矩阵 E 可以分解为 [t]×R。本质矩阵 E 是一个 3×3 的矩阵，它描述了两幅图像之间的几何关系。对于归一化图像坐标 x1 和 x2，满足以下约束：
$$
x_2^TEx_1=0
$$
​	其中 x1 和 x2 分别是第一幅和第二幅图像上的对应点。

​	在两幅图像之间，相机的相对位姿（旋转 R 和平移 t）定义了从一个坐标系到另一个坐标系的转换。假设第一幅图像的相机坐标系为 C1，第二幅图像的相机坐标系为 C2。则点在 C1 中的坐标 X1 和在 C2中的坐标 X2 之间的关系可以表示为：
$$
X_2=RX_1+t
$$

​	在归一化图像平面中，点的坐标 x1和 x2与其对应的三维坐标 X1 和 X2之间的关系通过相机内参矩阵 K 进行归一化。由于三维点 X 在两个图像坐标系之间的变换，可以得到以下关系：
$$
x_2 = Rx_1 + t
$$
​	通过点到直线的距离约束（极线几何），可以得到
$$
x_2^T[t]_xRx_1 = 0
$$


## 5.计算3D坐标和深度信息

​	要从特征检测和相机姿态估计继续实现视觉SLAM并获取障碍的深度信息，通常的步骤是通过视差（disparity）计算3D重建，具体步骤如下： 

1. 使用两个视角的匹配特征，计算深度信息。 
2. 使用三角测量法（Triangulation）得到3D点的坐标。

## 6.关键帧选择